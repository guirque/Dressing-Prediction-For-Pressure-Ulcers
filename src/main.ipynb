{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01a5e55d",
   "metadata": {},
   "source": [
    "## Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4ca3be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import PressureUlcers\n",
    "import torchvision.transforms as ttran\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from dataset import get_class_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a33dc2",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140c3a07",
   "metadata": {},
   "source": [
    "### Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50923d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "folder: Stage_III\n",
      "files: ['Stage_III_045.png', 'Stage_III_034.png', 'Stage_III_022.png', 'Stage_III_006.png', 'Stage_III_018.png', 'Stage_III_031.png', 'Stage_III_008.png', 'Stage_III_005.png', 'Stage_III_033.png', 'Stage_III_021.png', 'Stage_III_028.png', 'Stage_III_042.png', 'Stage_III_014.png', 'Stage_III_025.png', 'Stage_III_040.png', 'Stage_III_015.png', 'Stage_III_016.png', 'Stage_III_037.png', 'Stage_III_003.png', 'Stage_III_027.png', 'Stage_III_030.png', 'Stage_III_010.png', 'Stage_III_013.png', 'Stage_III_039.png', 'Stage_III_038.png', 'Stage_III_035.png', 'Stage_III_017.png', 'Stage_III_001.png', 'Stage_III_046.png', 'Stage_III_026.png', 'Stage_III_029.png', 'Stage_III_024.png', 'Stage_III_032.png', 'Stage_III_011.png', 'Stage_III_004.png', 'Stage_III_043.png', 'Stage_III_041.png', 'Stage_III_009.png', 'Stage_III_007.png', 'Stage_III_019.png', 'Stage_III_002.png', 'Stage_III_044.png', 'Stage_III_012.png', 'Stage_III_023.png', 'Stage_III_020.png', 'Stage_III_036.png']\n",
      "\n",
      "\n",
      "folder: SDTI\n",
      "files: ['SDTI_021.png', 'SDTI_012.png', 'SDTI_007.png', 'SDTI_018.png', 'SDTI_014.png', 'SDTI_011.png', 'SDTI_009.png', 'SDTI_015.png', 'SDTI_023.png', 'SDTI_016.png', 'SDTI_017.png', 'SDTI_002.png', 'SDTI_006.png', 'SDTI_019.png', 'SDTI_013.png', 'SDTI_020.png', 'SDTI_004.png', 'SDTI_022.png', 'SDTI_010.png', 'SDTI_005.png', 'SDTI_001.png', 'SDTI_008.png', 'SDTI_003.png']\n",
      "\n",
      "\n",
      "folder: Invalid\n",
      "files: ['Invalid_042.png', 'Invalid_016.png', 'Invalid_020.png', 'Invalid_043.png', 'Invalid_012.png', 'Invalid_019.png', 'Invalid_011.png', 'Invalid_004.png', 'Invalid_025.png', 'Invalid_010.png', 'Invalid_033.png', 'Invalid_046.png', 'Invalid_022.png', 'Invalid_023.png', 'Invalid_007.png', 'Invalid_027.png', 'Invalid_003.png', 'Invalid_008.png', 'Invalid_040.png', 'Invalid_038.png', 'Invalid_017.png', 'Invalid_036.png', 'Invalid_005.png', 'Invalid_031.png', 'Invalid_002.png', 'Invalid_024.png', 'Invalid_029.png', 'Invalid_014.png', 'Invalid_045.png', 'Invalid_006.png', 'Invalid_001.png', 'Invalid_035.png', 'Invalid_039.png', 'Invalid_030.png', 'Invalid_009.png', 'Invalid_032.png', 'Invalid_018.png', 'Invalid_034.png', 'Invalid_037.png', 'Invalid_047.png', 'Invalid_021.png', 'Invalid_013.png', 'Invalid_044.png', 'Invalid_026.png', 'Invalid_041.png', 'Invalid_015.png', 'Invalid_028.png']\n",
      "\n",
      "\n",
      "folder: Unstageable\n",
      "files: ['Unstageable_001.png', 'Unstageable_009.png', 'Unstageable_002.png', 'Unstageable_021.png', 'Unstageable_004.png', 'Unstageable_006.png', 'Unstageable_016.png', 'Unstageable_026.png', 'Unstageable_022.png', 'Unstageable_029.png', 'Unstageable_005.png', 'Unstageable_028.png', 'Unstageable_012.png', 'Unstageable_017.png', 'Unstageable_020.png', 'Unstageable_015.png', 'Unstageable_025.png', 'Unstageable_008.png', 'Unstageable_013.png', 'Unstageable_003.png', 'Unstageable_024.png', 'Unstageable_007.png', 'Unstageable_018.png', 'Unstageable_010.png', 'Unstageable_019.png', 'Unstageable_014.png', 'Unstageable_023.png', 'Unstageable_011.png', 'Unstageable_027.png']\n",
      "\n",
      "\n",
      "folder: Stage_I\n",
      "files: ['Stage_I_007.png', 'Stage_I_023.png', 'Stage_I_012.png', 'Stage_I_027.png', 'Stage_I_015.png', 'Stage_I_003.png', 'Stage_I_022.png', 'Stage_I_013.png', 'Stage_I_010.png', 'Stage_I_006.png', 'Stage_I_026.png', 'Stage_I_014.png', 'Stage_I_024.png', 'Stage_I_001.png', 'Stage_I_017.png', 'Stage_I_025.png', 'Stage_I_009.png', 'Stage_I_028.png', 'Stage_I_018.png', 'Stage_I_002.png', 'Stage_I_011.png', 'Stage_I_020.png', 'Stage_I_019.png', 'Stage_I_005.png', 'Stage_I_021.png', 'Stage_I_008.png', 'Stage_I_004.png', 'Stage_I_016.png']\n",
      "\n",
      "\n",
      "folder: Stage_IV\n",
      "files: ['Stage_IV_011.png', 'Stage_IV_010.png', 'Stage_IV_006.png', 'Stage_IV_024.png', 'Stage_IV_020.png', 'Stage_IV_007.png', 'Stage_IV_002.png', 'Stage_IV_013.png', 'Stage_IV_018.png', 'Stage_IV_030.png', 'Stage_IV_014.png', 'Stage_IV_004.png', 'Stage_IV_017.png', 'Stage_IV_008.png', 'Stage_IV_032.png', 'Stage_IV_029.png', 'Stage_IV_005.png', 'Stage_IV_001.png', 'Stage_IV_026.png', 'Stage_IV_022.png', 'Stage_IV_025.png', 'Stage_IV_023.png', 'Stage_IV_016.png', 'Stage_IV_015.png', 'Stage_IV_027.png', 'Stage_IV_003.png', 'Stage_IV_028.png', 'Stage_IV_021.png', 'Stage_IV_009.png', 'Stage_IV_012.png', 'Stage_IV_031.png', 'Stage_IV_019.png']\n",
      "\n",
      "\n",
      "folder: Stage_II\n",
      "files: ['Stage_II_012.png', 'Stage_II_019.png', 'Stage_II_020.png', 'Stage_II_002.png', 'Stage_II_032.png', 'Stage_II_049.png', 'Stage_II_030.png', 'Stage_II_024.png', 'Stage_II_003.png', 'Stage_II_016.png', 'Stage_II_023.png', 'Stage_II_043.png', 'Stage_II_045.png', 'Stage_II_015.png', 'Stage_II_014.png', 'Stage_II_007.png', 'Stage_II_011.png', 'Stage_II_047.png', 'Stage_II_053.png', 'Stage_II_001.png', 'Stage_II_021.png', 'Stage_II_005.png', 'Stage_II_028.png', 'Stage_II_006.png', 'Stage_II_034.png', 'Stage_II_008.png', 'Stage_II_042.png', 'Stage_II_048.png', 'Stage_II_041.png', 'Stage_II_013.png', 'Stage_II_025.png', 'Stage_II_018.png', 'Stage_II_029.png', 'Stage_II_038.png', 'Stage_II_022.png', 'Stage_II_052.jpeg', 'Stage_II_035.png', 'Stage_II_004.png', 'Stage_II_044.png', 'Stage_II_027.png', 'Stage_II_040.png', 'Stage_II_009.png', 'Stage_II_036.png', 'Stage_II_017.png', 'Stage_II_051.jpeg', 'Stage_II_039.png', 'Stage_II_031.png', 'Stage_II_026.png', 'Stage_II_033.png', 'Stage_II_037.png', 'Stage_II_046.png', 'Stage_II_050.jpeg', 'Stage_II_010.png']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "IMG_SIZE = (448, 448) # (448, 448)\n",
    "\n",
    "JSON_MAPPING_PATH = '../data/unzipped/notes.json'\n",
    "transform = ttran.Compose([ttran.Resize(IMG_SIZE), ttran.ToTensor(), ttran.Normalize((0, 0, 0), (1, 1, 1))]) # ttran.ColorJitter()\n",
    "\n",
    "\n",
    "class_map = get_class_map(JSON_MAPPING_PATH)\n",
    "\n",
    "ds = PressureUlcers('../data/images', '../data/unzipped/labels', transform, grid_size=7, num_bounding_boxes=2, img_size=IMG_SIZE, num_classes=len(class_map))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52aa366",
   "metadata": {},
   "source": [
    "## Setting Up Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44944ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import device\n",
    "\n",
    "torch_device = device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5870d05e",
   "metadata": {},
   "source": [
    "## Creating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52f586da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import YOLOModel\n",
    "from simple_yolo_model import SimpleYOLOModel\n",
    "model = SimpleYOLOModel(num_classes=len(class_map), img_size=IMG_SIZE[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bbaf87",
   "metadata": {},
   "source": [
    "## Testing For Single Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fa888e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 7, 46])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_img, labels = ds[120]\n",
    "# Putting it in a batch of its own\n",
    "tensor_img = tensor_img.reshape(1, tensor_img.shape[0], tensor_img.shape[1], tensor_img.shape[2] )\n",
    "\n",
    "iter_dl = iter(DataLoader(dataset=tensor_img, batch_size=1, shuffle=True)) \n",
    "\n",
    "# About using dataloader and having it convert [H, W, C] to [C, H, W] (IAmIronMan's comment)\n",
    "# https://discuss.pytorch.org/t/runtimeerror-given-groups-1-weight-of-size-64-3-7-7-expected-input-3-1-224-224-to-have-3-channels-but-got-1-channels-instead/30153/8\n",
    "\n",
    "model = model.eval()\n",
    "\n",
    "result = model(next(iter_dl))\n",
    "\n",
    "result.shape\n",
    "# 7x7 grid. Each value is composed of 5 bounding boxes (each with x, y, w, h) \n",
    "# and class probabilities for each of the 20 classes\n",
    "# Output size = S x S x (B * 5 + C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79431b14",
   "metadata": {},
   "source": [
    "## Setting Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12438a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.ultralytics.com/glossary/intersection-over-union-iou\n",
    "# A threshold determines if the prediction is acceptable\n",
    "\n",
    "def IOU(boxes, ground_truth_boxes):\n",
    "    \"\"\"\n",
    "        Intersection over Union implementation.\n",
    "        It is used to evaluate how well the predicted bounding box is in comparison with the ground truth.\n",
    "\n",
    "        $ IOU = (Intersection)/(Union)\n",
    "\n",
    "        ## Args\n",
    "\n",
    "        - *boxes*: array-like of boxes. Each box is a list of c, x, y, w, h.\n",
    "        - *ground_truth_boxes*: array-like of boxes. Each box is a list of c, x, y, w, h\n",
    "    \"\"\"\n",
    "\n",
    "    # Between two boxes\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcd0eeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.ops import box_iou\n",
    "from yolo_loss import yolo_loss\n",
    "\n",
    "LEARNING_RATE = 0.001\n",
    "SCHEDULER_STEP_SIZE = 10\n",
    "SCHEDULER_GAMMA = 0.1\n",
    "BATCH_SIZE = 25 # about 25 is the limit for SimpleYoloModel with our current gpu\n",
    "NUM_EPOCHS = 15\n",
    "\n",
    "optimizer = torch.optim.Adadelta(params=model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, SCHEDULER_STEP_SIZE, SCHEDULER_GAMMA)\n",
    "loss = yolo_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cdd91b",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b829c34",
   "metadata": {},
   "source": [
    "### Splitting into Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8be76a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# train_indices, test_indices = train_test_split(range(len(ds)), test_size=0.2, random_state=42)\n",
    "\n",
    "train_ds, test_ds = random_split(ds, [0.8, 0.2], torch.Generator().manual_seed(42)) # https://stackoverflow.com/questions/50544730/how-do-i-split-a-custom-dataset-into-training-and-test-datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc52f01c",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c5e1fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 1/15 [00:06<01:34,  6.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Loss: 2394.027702331543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 2/15 [00:13<01:24,  6.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss: 1599.3828506469727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 3/15 [00:19<01:16,  6.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Loss: 1562.3222923278809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 4/15 [00:25<01:09,  6.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Loss: 1561.098861694336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 5/15 [00:31<01:03,  6.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Loss: 1531.6803665161133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 6/15 [00:38<00:56,  6.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Loss: 1533.982219696045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 7/15 [00:44<00:50,  6.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 | Loss: 1559.7909088134766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 8/15 [00:50<00:44,  6.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 | Loss: 1558.1504096984863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 9/15 [00:57<00:37,  6.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 | Loss: 1557.1004791259766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 10/15 [01:03<00:31,  6.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 | Loss: 1561.8952560424805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 11/15 [01:09<00:25,  6.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | Loss: 1578.581901550293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 12/15 [01:15<00:18,  6.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 | Loss: 1587.7446899414062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 13/15 [01:22<00:12,  6.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 | Loss: 1516.98237991333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 14/15 [01:28<00:06,  6.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 | Loss: 1591.1847686767578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [01:34<00:00,  6.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 | Loss: 1536.5187301635742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import math\n",
    "from torch.nn.utils import clip_grad_norm_, clip_grad_value_\n",
    "\n",
    "NUM_BATCHES = math.ceil(len(train_ds) / BATCH_SIZE)\n",
    "\n",
    "model.train()\n",
    "\n",
    "model.to(torch_device)\n",
    "\n",
    "# For each epoch\n",
    "for epoch in tqdm(range(NUM_EPOCHS)):\n",
    "    \n",
    "    # Load dataset\n",
    "    iter_dl = iter(DataLoader(dataset=train_ds, batch_size=BATCH_SIZE, shuffle=True))\n",
    "\n",
    "    final_loss = 0\n",
    "\n",
    "    # For each batch\n",
    "    for batch_i in range(NUM_BATCHES):\n",
    "\n",
    "        # Load data\n",
    "        img_data, labels = next(iter_dl)\n",
    "\n",
    "        # Put it on GPU\n",
    "        img_data = img_data.to(torch_device)\n",
    "        labels = labels.to(torch_device)\n",
    "\n",
    "        # Make predictions\n",
    "        predictions = model(img_data)\n",
    "\n",
    "        # Calculate Loss\n",
    "        loss_res = yolo_loss(labels, predictions, num_classes=model.num_classes)\n",
    "\n",
    "        # Optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss_res.backward()\n",
    "        # clip gradient to avoid exploding gradients\n",
    "        #clip_grad_norm_(model.parameters(), 1)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        final_loss += loss_res.item() # Use of .item (pandas007): https://stackoverflow.com/questions/59129812/how-to-avoid-cuda-out-of-memory-in-pytorch\n",
    "    print(f'Epoch {epoch} | Loss: {final_loss}')\n",
    "    \n",
    "# https://discuss.pytorch.org/t/why-my-model-returns-nan/24329/6\n",
    "# https://medium.com/biased-algorithms/guide-to-gradient-clipping-in-pytorch-f1db24ea08a2\n",
    "# RuntimeError: The size of tensor a (23) must match the size of tensor b (0) at non-singleton dimension 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modeling_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
